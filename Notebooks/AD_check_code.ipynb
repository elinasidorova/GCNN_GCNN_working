{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cairne/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## make donor model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cairne/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/cairne/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: Output/logP_data_logP_regression_2022_08_16_12_47_45/fold_1/logs/default\n",
      "\n",
      "  | Name            | Type              | Params\n",
      "------------------------------------------------------\n",
      "0 | conv_actf       | LeakyReLU         | 0     \n",
      "1 | linear_actf     | LeakyReLU         | 0     \n",
      "2 | conv_sequential | Sequential_7afd7d | 544 K \n",
      "3 | lin_sequential  | Sequential        | 131 K \n",
      "------------------------------------------------------\n",
      "676 K     Trainable params\n",
      "0         Non-trainable params\n",
      "676 K     Total params\n",
      "2.704     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cairne/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (0x75 and 128x64)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     18\u001B[0m folds, test_loader \u001B[38;5;241m=\u001B[39m train_test_valid_split(featurized_train, n_split, test_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2000\u001B[39m)\n\u001B[1;32m     20\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ModelTrainer(MolGraphNet, folds, test_loader, output_path, output_mark, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregression\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_cv_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PythonProj/SmartChemDesign/mol_torch_model/Source/trainer.py:80\u001B[0m, in \u001B[0;36mModelTrainer.train_cv_models\u001B[0;34m(self, pretrained_folder, layers_to_freeze)\u001B[0m\n\u001B[1;32m     76\u001B[0m         model\u001B[38;5;241m.\u001B[39mload_state_dict(\n\u001B[1;32m     77\u001B[0m             torch\u001B[38;5;241m.\u001B[39mload(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(pretrained_folder, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfold_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_fold \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[1;32m     78\u001B[0m         model\u001B[38;5;241m.\u001B[39mfreeze_layers(layers_to_freeze)\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_fold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_fold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m mean_valid_pred \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msingle_fold_predict(m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_valid_data[i][\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     83\u001B[0m                                                    \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels)]), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     84\u001B[0m valid_true \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(np\u001B[38;5;241m.\u001B[39marray([Batch\u001B[38;5;241m.\u001B[39mfrom_data_list(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_valid_data[i][\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdataset)\u001B[38;5;241m.\u001B[39my \u001B[38;5;28;01mfor\u001B[39;00m i, val\n\u001B[1;32m     85\u001B[0m                                       \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_valid_data)]))\n",
      "File \u001B[0;32m~/PythonProj/SmartChemDesign/mol_torch_model/Source/trainer.py:107\u001B[0m, in \u001B[0;36mModelTrainer.train_model\u001B[0;34m(self, model, train_fold, valid_fold, fold_num, epochs)\u001B[0m\n\u001B[1;32m    104\u001B[0m es_callback \u001B[38;5;241m=\u001B[39m EarlyStopping(patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mes_patience, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    106\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(callbacks\u001B[38;5;241m=\u001B[39m[es_callback], log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, max_epochs\u001B[38;5;241m=\u001B[39mepochs, logger\u001B[38;5;241m=\u001B[39mtb_logger)\n\u001B[0;32m--> 107\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_fold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_fold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mappend(model)\n\u001B[1;32m    110\u001B[0m current_folder \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmain_folder, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfold_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold_num \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001B[0m\n\u001B[1;32m    735\u001B[0m     rank_zero_deprecation(\n\u001B[1;32m    736\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    737\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    738\u001B[0m     )\n\u001B[1;32m    739\u001B[0m     train_dataloaders \u001B[38;5;241m=\u001B[39m train_dataloader\n\u001B[0;32m--> 740\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[0;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001B[39;00m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;124;03mas all errors should funnel through them\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001B[39;00m\n\u001B[1;32m    683\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    684\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 685\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001B[39;00m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    775\u001B[0m \u001B[38;5;66;03m# TODO: ckpt_path only in v1.7\u001B[39;00m\n\u001B[1;32m    776\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[0;32m--> 777\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[1;32m   1198\u001B[0m \u001B[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001B[39;00m\n\u001B[0;32m-> 1199\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001B[39;00m\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_dispatch()\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001B[0m, in \u001B[0;36mTrainer._dispatch\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1277\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_type_plugin\u001B[38;5;241m.\u001B[39mstart_predicting(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1279\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_type_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_training\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001B[0m, in \u001B[0;36mTrainingTypePlugin.start_training\u001B[0;34m(self, trainer)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart_training\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.Trainer\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;66;03m# double dispatch to initiate the training loop\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_results \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001B[0m, in \u001B[0;36mTrainer.run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1287\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[1;32m   1288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1311\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_global_zero \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogress_bar_callback \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprogress_bar_callback\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[0;32m-> 1311\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1313\u001B[0m \u001B[38;5;66;03m# enable train mode\u001B[39;00m\n\u001B[1;32m   1314\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1375\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self, ref_model)\u001B[0m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m-> 1375\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_hook(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1379\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 145\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrestarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:110\u001B[0m, in \u001B[0;36mEvaluationLoop.advance\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_fetcher \u001B[38;5;241m=\u001B[39m dataloader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mget_profiled_dataloader(\n\u001B[1;32m    106\u001B[0m     dataloader, dataloader_idx\u001B[38;5;241m=\u001B[39mdataloader_idx\n\u001B[1;32m    107\u001B[0m )\n\u001B[1;32m    108\u001B[0m dl_max_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_batches[dataloader_idx]\n\u001B[0;32m--> 110\u001B[0m dl_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_max_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_dataloaders\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# store batch level output per dataloader\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs\u001B[38;5;241m.\u001B[39mappend(dl_outputs)\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:145\u001B[0m, in \u001B[0;36mLoop.run\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 145\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrestarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:122\u001B[0m, in \u001B[0;36mEvaluationEpochLoop.advance\u001B[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;66;03m# lightning module methods\u001B[39;00m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluation_step_and_end\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 122\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_step_end(output)\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:217\u001B[0m, in \u001B[0;36mEvaluationEpochLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx)\u001B[0m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 217\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:239\u001B[0m, in \u001B[0;36mAccelerator.validation_step\u001B[0;34m(self, step_kwargs)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m\"\"\"The actual validation step.\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \n\u001B[1;32m    236\u001B[0m \u001B[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001B[39;00m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mval_step_context():\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_type_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_kwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:219\u001B[0m, in \u001B[0;36mTrainingTypePlugin.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PythonProj/SmartChemDesign/mol_torch_model/Source/model.py:131\u001B[0m, in \u001B[0;36mMolGraphNet.validation_step\u001B[0;34m(self, val_batch, batch_idx)\u001B[0m\n\u001B[1;32m    129\u001B[0m X, y \u001B[38;5;241m=\u001B[39m val_batch, val_batch\u001B[38;5;241m.\u001B[39my\n\u001B[1;32m    130\u001B[0m x, edge_index, batch \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mx, X\u001B[38;5;241m.\u001B[39medge_index, X\u001B[38;5;241m.\u001B[39mbatch\n\u001B[0;32m--> 131\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    132\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(y, logits)\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, loss)\n",
      "File \u001B[0;32m~/PythonProj/SmartChemDesign/mol_torch_model/Source/model.py:101\u001B[0m, in \u001B[0;36mMolGraphNet.forward\u001B[0;34m(self, x, edge_index, batch, num_classes)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index, batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    100\u001B[0m     edge_index, _ \u001B[38;5;241m=\u001B[39m add_self_loops(edge_index, num_nodes\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m--> 101\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_sequential\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m     x \u001B[38;5;241m=\u001B[39m gap(x, batch)\n\u001B[1;32m    103\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin_sequential(x)\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/tmp/cairne_pyg/tmpj0tf25wy.py:18\u001B[0m, in \u001B[0;36mSequential_7afd7d.forward\u001B[0;34m(self, x, edge_index)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index):\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule_0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_1(x, edge_index)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/tmp/cairne_pyg/tmp7yoy53iw.py:18\u001B[0m, in \u001B[0;36mSequential_7afaf5.forward\u001B[0;34m(self, x, edge_index)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index):\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule_0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_1(x)\n\u001B[1;32m     20\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_2(x)\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch_geometric/nn/conv/mf_conv.py:101\u001B[0m, in \u001B[0;36mMFConv.forward\u001B[0;34m(self, x, edge_index, size)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (lin_l, lin_r) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlins_l, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlins_r)):\n\u001B[1;32m    100\u001B[0m     idx \u001B[38;5;241m=\u001B[39m (deg \u001B[38;5;241m==\u001B[39m i)\u001B[38;5;241m.\u001B[39mnonzero()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 101\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mlin_l\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x_r \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    104\u001B[0m         r \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m lin_r(x_r\u001B[38;5;241m.\u001B[39mindex_select(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim, idx))\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:118\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m        x (Tensor): The features.\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (0x75 and 128x64)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Source\")\n",
    "\n",
    "from Source.trainer import ModelTrainer\n",
    "from Source.model import MolGraphNet\n",
    "from Source.data import train_test_valid_split\n",
    "from Source.mol_featurizer import featurize_sdf\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "path_to_sdf = \"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Data/logS_logP.sdf\"\n",
    "valuenames = [\"logP\"]\n",
    "output_path = \"Output\"\n",
    "n_split = 5\n",
    "output_mark = f\"logP_data_{valuenames[0]}\"\n",
    "\n",
    "featurized_train = featurize_sdf(path_to_sdf, valuenames)\n",
    "folds, test_loader = train_test_valid_split(featurized_train, n_split, test_ratio=0.1, batch_size=2000)\n",
    "\n",
    "trainer = ModelTrainer(MolGraphNet, folds, test_loader, output_path, output_mark, mode=\"regression\")\n",
    "trainer.train_cv_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## make acceptor model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cairne/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MolGraphNet:\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.0.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.1.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.2.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.3.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.4.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.5.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.5.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.6.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.7.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.7.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.8.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.8.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.9.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.10.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.10.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.0.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.1.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.2.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.3.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.4.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.5.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.6.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.7.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.8.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.9.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.10.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.0.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.1.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.3.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.4.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.5.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.5.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.6.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.6.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.7.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.8.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.8.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.9.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.9.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.10.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.10.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.0.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.1.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.3.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.4.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.5.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.6.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.8.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.9.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.10.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for lin_sequential.0.0.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for lin_sequential.0.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lin_sequential.1.0.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for lin_sequential.1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lin_sequential.2.0.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     22\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader([val \u001B[38;5;28;01mfor\u001B[39;00m i, val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(featurized_test)], batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     24\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ModelTrainer(MolGraphNet, folds, test_loader, output_path, output_mark, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregression\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 25\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_cv_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Output/AD_check_models/Results_Eu_data_logP_regression_2022_08_15_22_48_47\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mlayers_to_freeze\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PythonProj/SmartChemDesign/mol_torch_model/Source/trainer.py:76\u001B[0m, in \u001B[0;36mModelTrainer.train_cv_models\u001B[0;34m(self, pretrained_folder, layers_to_freeze)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pretrained_folder:\n\u001B[1;32m     75\u001B[0m     best_fold \u001B[38;5;241m=\u001B[39m get_best_fold(pretrained_folder)\n\u001B[0;32m---> 76\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfold_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mbest_fold\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m     model\u001B[38;5;241m.\u001B[39mfreeze_layers(layers_to_freeze)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_model(model, train_fold, valid_fold, i, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs)\n",
      "File \u001B[0;32m~/anaconda3/envs/mol_torch_gcnn/lib/python3.9/site-packages/torch/nn/modules/module.py:1497\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   1492\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   1493\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1494\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1497\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1498\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for MolGraphNet:\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.0.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.1.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.2.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.3.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.4.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.4.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.5.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.5.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.6.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.6.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.7.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.7.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.8.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.8.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.9.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.10.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_l.10.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.0.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.1.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.2.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.3.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.4.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.5.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.6.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.7.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.8.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.9.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_0.module_0.lins_r.10.weight: copying a param with shape torch.Size([256, 75]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.0.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.1.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.3.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.4.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.4.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.5.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.5.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.6.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.6.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.7.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.8.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.8.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.9.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.9.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.10.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_l.10.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.0.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.1.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.3.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.4.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.5.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.6.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.7.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.8.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.9.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for conv_sequential.module_1.module_0.lins_r.10.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for lin_sequential.0.0.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for lin_sequential.0.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lin_sequential.1.0.weight: copying a param with shape torch.Size([64, 64]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for lin_sequential.1.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lin_sequential.2.0.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([1, 256])."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Source\")\n",
    "\n",
    "from Source.trainer import ModelTrainer\n",
    "from Source.model import MolGraphNet\n",
    "from Source.data import train_test_valid_split\n",
    "from Source.mol_featurizer import featurize_sdf\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "path_to_sdf = \"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Data/Am_ML_logS_logP_train.sdf\"\n",
    "test_sdf = \"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Data/Am_ML_logS_logP_test.sdf\"\n",
    "valuenames = [\"logS\"]\n",
    "output_path = \"Output/AD_check_models\"\n",
    "n_split = 5\n",
    "output_mark = f\"Am_data_{valuenames[0]}\"\n",
    "\n",
    "featurized_train = featurize_sdf(path_to_sdf, valuenames)\n",
    "folds = train_test_valid_split(featurized_train, n_split, test_ratio=0, batch_size=1, subsample_size=False, return_test=False)\n",
    "\n",
    "featurized_test = featurize_sdf(test_sdf, valuenames)\n",
    "test_loader = DataLoader([val for i, val in enumerate(featurized_test)], batch_size=1)\n",
    "\n",
    "trainer = ModelTrainer(MolGraphNet, folds, test_loader, output_path, output_mark, mode=\"regression\")\n",
    "trainer.train_cv_models(pretrained_folder=\"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Output/AD_check_models/Results_Eu_data_logP_regression_2022_08_15_22_48_47\",\n",
    "                        layers_to_freeze=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make metals models for donor/acceptor pairs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Source\")\n",
    "\n",
    "from Source.trainer import ModelTrainer\n",
    "from Source.model import MolGraphNet\n",
    "from Source.data import train_test_valid_split\n",
    "from Source.mol_featurizer import featurize_sdf\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "metals = [\"Al\", \"Cu\", \"Cd\", \"Mg\", \"Pb\", \"Ni\", \"Zn\"]\n",
    "valuenames = [\"logK\"]\n",
    "output_path = \"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Output/add_metals_for_AD\"\n",
    "n_split = 5\n",
    "for metal in metals:\n",
    "    output_mark = f\"{metal}_data_{valuenames[0]}\"\n",
    "    path_to_sdf = f\"/home/cairne/PythonProj/SmartChemDesign/mol_torch_model/Data/metals_sdf_csv/converted/{metal}_ML.sdf\"\n",
    "\n",
    "    featurized_train = featurize_sdf(path_to_sdf, valuenames)\n",
    "    folds, test_loader = train_test_valid_split(featurized_train, n_split, test_ratio=0.2, batch_size=30)\n",
    "\n",
    "    trainer = ModelTrainer(MolGraphNet, folds, test_loader, output_path, output_mark, mode=\"regression\")\n",
    "    trainer.train_cv_models()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}